#!/usr/bin/env python3
"""
Main training script for DP DistilBERT model.
Fast, efficient, with proper logging and early stopping.
"""

import jax
import jax.numpy as jnp
import numpy as np
import pandas as pd
from pathlib import Path
import json
import time
import argparse
from datetime import datetime
from typing import Dict, List, Tuple

# Import our modules
from src.models.distilbert_dp import DistilBertDP, DPTrainer, TrainingState
import src.evaluation.plot_utils as plot_utils


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Train DP DistilBERT model")
    
    parser.add_argument("--epsilon", type=float, default=8.0,
                       help="Privacy budget (Œµ)")
    parser.add_argument("--model_name", type=str, default="distilbert_epsilon_8",
                       help="Model name for saving")
    parser.add_argument("--epochs", type=int, default=5,
                       help="Number of epochs")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="Batch size")
    parser.add_argument("--learning_rate", type=float, default=5e-5,
                       help="Learning rate")
    parser.add_argument("--max_seq_length", type=int, default=128,
                       help="Maximum sequence length")
    parser.add_argument("--output_dir", type=str, default="outputs/models",
                       help="Output directory")
    parser.add_argument("--early_stopping_patience", type=int, default=2,
                       help="Early stopping patience")
    
    return parser.parse_args()


def load_and_preprocess_data(max_seq_length: int = 128) -> Tuple[Dict, Dict, Dict]:
    """
    Load and preprocess data for training.
    
    Args:
        max_seq_length: Maximum sequence length
        
    Returns:
        Tuple of (train_data, val_data, test_data) dictionaries
    """
    print("üìÇ Loading data...")
    
    # Load preprocessed splits
    train_df = pd.read_csv("data/processed/train.csv")
    val_df = pd.read_csv("data/processed/val.csv")
    test_df = pd.read_csv("data/processed/test.csv")
    
    # Convert labels from string to list
    def parse_labels(labels_str):
        if isinstance(labels_str, str):
            return eval(labels_str)
        return labels_str
    
    train_df['labels'] = train_df['labels'].apply(parse_labels)
    val_df['labels'] = val_df['labels'].apply(parse_labels)
    test_df['labels'] = test_df['labels'].apply(parse_labels)
    
    # Simple tokenization (in practice, use DistilBERT tokenizer)
    def tokenize_text(text):
        # Simplified tokenization - split by space
        tokens = text.split()
        # Pad/truncate to max_seq_length
        if len(tokens) > max_seq_length:
            tokens = tokens[:max_seq_length]
        else:
            tokens = tokens + ['[PAD]'] * (max_seq_length - len(tokens))
        return tokens
    
    # Create dataset dictionaries
    def create_dataset(df):
        texts = df['text'].tolist()
        labels = df['labels'].tolist()
        
        # Tokenize (simplified)
        tokenized = [tokenize_text(text) for text in texts]
        
        # Convert to indices (simplified - use vocab mapping in practice)
        # For demo, use random indices
        input_ids = np.random.randint(0, 30000, (len(df), max_seq_length))
        
        # Create attention mask (1 for real tokens, 0 for padding)
        attention_mask = np.ones((len(df), max_seq_length))
        
        # Convert labels to numpy array (pad to max_seq_length)
        label_array = np.zeros((len(df), max_seq_length), dtype=np.int32)
        for i, label_list in enumerate(labels):
            if isinstance(label_list, list):
                length = min(len(label_list), max_seq_length)
                # Convert BIO tags to indices: O=0, B=1, I=2
                for j in range(length):
                    if label_list[j] == 'O':
                        label_array[i, j] = 0
                    elif label_list[j].startswith('B-'):
                        label_array[i, j] = 1
                    elif label_list[j].startswith('I-'):
                        label_array[i, j] = 2
        
        return {
            'input_ids': jnp.array(input_ids),
            'attention_mask': jnp.array(attention_mask),
            'labels': jnp.array(label_array)
        }
    
    train_data = create_dataset(train_df)
    val_data = create_dataset(val_df)
    test_data = create_dataset(test_df)
    
    print(f"‚úÖ Data loaded: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}")
    
    return train_data, val_data, test_data


def create_batches(data: Dict, batch_size: int) -> List[Tuple]:
    """
    Create batches from dataset.
    
    Args:
        data: Dataset dictionary
        batch_size: Batch size
        
    Returns:
        List of batches
    """
    n_samples = data['input_ids'].shape[0]
    batches = []
    
    for i in range(0, n_samples, batch_size):
        batch = (
            data['input_ids'][i:i+batch_size],
            data['attention_mask'][i:i+batch_size],
            data['labels'][i:i+batch_size]
        )
        batches.append(batch)
    
    return batches


def train_model(args, train_data: Dict, val_data: Dict) -> Tuple[TrainingState, Dict]:
    """
    Train the DP model.
    
    Args:
        args: Command line arguments
        train_data: Training data
        val_data: Validation data
        
    Returns:
        Tuple of (final_state, training_history)
    """
    print(f"\nüèãÔ∏è‚Äç‚ôÇÔ∏è Starting training (Œµ={args.epsilon})...")
    
    # Initialize model
    model = DistilBertDP(epsilon=args.epsilon)
    trainer = DPTrainer(
        model=model,
        learning_rate=args.learning_rate,
        epsilon=args.epsilon,
        batch_size=args.batch_size
    )
    
    # Initialize random key
    rng = jax.random.PRNGKey(42)
    rng, init_rng = jax.random.split(rng)
    
    # Create dummy input for initialization
    dummy_input = jnp.ones((2, args.max_seq_length), dtype=jnp.int32)
    dummy_mask = jnp.ones_like(dummy_input)
    
    # Initialize training state
    state = trainer.init_state(init_rng, dummy_input, dummy_mask)
    
    # Create batches
    train_batches = create_batches(train_data, args.batch_size)
    val_batch = create_batches(val_data, args.batch_size)[0]  # Single batch for validation
    
    # Training history
    history = {
        'train_loss': [],
        'train_accuracy': [],
        'val_loss': [],
        'val_accuracy': [],
        'val_f1': [],
        'epochs': [],
        'timestamps': []
    }
    
    # Early stopping
    best_val_f1 = 0.0
    patience_counter = 0
    best_state = None
    
    # Training loop
    for epoch in range(args.epochs):
        epoch_start = time.time()
        
        # Training
        epoch_loss = 0.0
        epoch_accuracy = 0.0
        
        for batch in train_batches:
            state, metrics = trainer.train_step(state, batch)
            epoch_loss += metrics['loss']
            epoch_accuracy += metrics['accuracy']
        
        # Average metrics
        avg_loss = epoch_loss / len(train_batches)
        avg_accuracy = epoch_accuracy / len(train_batches)
        
        # Validation
        val_metrics = trainer.evaluate(state.params, val_batch)
        
        # Record history
        history['train_loss'].append(float(avg_loss))
        history['train_accuracy'].append(float(avg_accuracy))
        history['val_loss'].append(float(val_metrics.get('loss', 0.0)))
        history['val_accuracy'].append(float(val_metrics['accuracy']))
        history['val_f1'].append(float(val_metrics['f1_score']))
        history['epochs'].append(epoch + 1)
        history['timestamps'].append(time.time())
        
        # Print progress
        epoch_time = time.time() - epoch_start
        print(f"  Epoch {epoch+1}/{args.epochs} | "
              f"Time: {epoch_time:.1f}s | "
              f"Train Loss: {avg_loss:.4f} | "
              f"Train Acc: {avg_accuracy:.4f} | "
              f"Val Acc: {val_metrics['accuracy']:.4f} | "
              f"Val F1: {val_metrics['f1_score']:.4f}")
        
        # Early stopping check
        current_f1 = val_metrics['f1_score']
        if current_f1 > best_val_f1:
            best_val_f1 = current_f1
            best_state = state
            patience_counter = 0
            print(f"    ‚Ü≥ New best F1: {best_val_f1:.4f}")
        else:
            patience_counter += 1
            if patience_counter >= args.early_stopping_patience:
                print(f"    ‚Ü≥ Early stopping triggered (patience: {patience_counter})")
                break
    
    # Use best state if early stopped
    final_state = best_state if best_state else state
    
    # Compute privacy cost
    privacy_cost = model.compute_privacy_cost(
        steps=state.step,
        batch_size=args.batch_size,
        dataset_size=train_data['input_ids'].shape[0]
    )
    
    history['privacy_cost'] = privacy_cost
    history['total_steps'] = state.step
    
    return final_state, history


def save_training_results(state: TrainingState, history: Dict, args):
    """Save training results and model."""
    output_dir = Path(args.output_dir) / args.model_name
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Save model checkpoint
    checkpoint_path = output_dir / "model_checkpoint.pkl"
    from src.models.distilbert_dp import save_checkpoint
    save_checkpoint(state, str(checkpoint_path), history)
    
    # Save training history
    history_path = output_dir / "training_history.json"
    with open(history_path, 'w') as f:
        # Convert numpy types to Python types
        serializable_history = {}
        for key, value in history.items():
            if key == 'privacy_cost':
                serializable_history[key] = {k: float(v) for k, v in value.items()}
            else:
                serializable_history[key] = [float(v) if isinstance(v, (np.float32, np.float64)) else v 
                                           for v in value]
        
        json.dump(serializable_history, f, indent=2)
    
    # Save configuration
    config = {
        'epsilon': args.epsilon,
        'model_name': args.model_name,
        'epochs_trained': len(history['epochs']),
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'max_seq_length': args.max_seq_length,
        'total_steps': history['total_steps'],
        'privacy_cost': history['privacy_cost'],
        'best_val_f1': max(history['val_f1']) if history['val_f1'] else 0.0,
        'timestamp': datetime.now().isoformat()
    }
    
    config_path = output_dir / "training_config.json"
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"\nüíæ Results saved to: {output_dir}")
    print(f"   - Model checkpoint: {checkpoint_path}")
    print(f"   - Training history: {history_path}")
    print(f"   - Configuration: {config_path}")
    
    return output_dir


def main():
    """Main training pipeline."""
    args = parse_args()
    
    print("="*70)
    print(f"üöÄ DP DISTILBERT TRAINING")
    print("="*70)
    print(f"  Œµ (epsilon): {args.epsilon}")
    print(f"  Model: {args.model_name}")
    print(f"  Epochs: {args.epochs}")
    print(f"  Batch size: {args.batch_size}")
    print("="*70)
    
    start_time = time.time()
    
    try:
        # Load data
        train_data, val_data, test_data = load_and_preprocess_data(args.max_seq_length)
        
        # Train model
        final_state, history = train_model(args, train_data, val_data)
        
        # Save results
        output_dir = save_training_results(final_state, history, args)
        
        # Generate training plots
        plot_utils.generate_training_plots(history, output_dir)
        
        # Print summary
        training_time = time.time() - start_time
        print(f"\nüìã TRAINING SUMMARY")
        print("="*70)
        print(f"  Total time: {training_time:.1f}s")
        print(f"  Total steps: {history['total_steps']}")
        print(f"  Best Val F1: {max(history['val_f1']):.4f}")
        print(f"  Privacy cost: Œµ={history['privacy_cost']['epsilon']:.2f}")
        print(f"  Output directory: {output_dir}")
        print("="*70)
        
    except Exception as e:
        print(f"\n‚ùå Training failed with error: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
